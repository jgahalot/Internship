{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b42c0bc6",
   "metadata": {},
   "source": [
    "# WIKI TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d18539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "page_wiki=requests.get(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
    "soup_wiki=BeautifulSoup(page.text,'html.parser')\n",
    "titles_wiki=soup_wiki.find_all(['h1','h2','h3','h4','h5','h6'])\n",
    "print(titles_wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1c894",
   "metadata": {},
   "source": [
    "# IMDB TOP RATED MOVIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d01072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "page_imdb= requests.get('https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc&view=simple')\n",
    "soup_imdb= BeautifulSoup(page_imdb.text, 'html.parser')\n",
    "\n",
    "\n",
    "movie_list=[]\n",
    "for i in soup_imdb.find_all('span',class_=\"lister-item-header\"):\n",
    "    movie_list.append(i.text.replace('\\n',''))\n",
    "    \n",
    "rating_list=[]\n",
    "for i in soup_imdb.find_all('div',class_=\"col-imdb-rating\"):\n",
    "    rating_list.append(i.text.replace('\\n',''))\n",
    "    \n",
    "    \n",
    "release_year=[]\n",
    "for i in soup_imdb.find_all('span',class_=\"lister-item-year text-muted unbold\"):\n",
    "    release_year.append(i.text)\n",
    "\n",
    "data={'Movie':movie_list,'Rating':rating_list,'Release Year':release_year}\n",
    "\n",
    "df=pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc26acc",
   "metadata": {},
   "source": [
    "# IMDB TOP RATED HINDI MOVIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d00343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "page_imdb_hindi= requests.get('https://www.imdb.com/list/ls009997493/')\n",
    "soup_imdb_hindi= BeautifulSoup(page_imdb_hindi.text, 'html.parser')\n",
    "\n",
    "\n",
    "movie_list_hindi=[]\n",
    "for i in soup_imdb_hindi.find_all('h3',class_=\"lister-item-header\"):\n",
    "    movie_list_hindi.append(i.text.replace('\\n',''))\n",
    "    \n",
    "    \n",
    "rating_list_hindi=[]\n",
    "for i in soup_imdb_hindi.find_all('div',class_=\"ipl-rating-star small\"):\n",
    "    rating_list_hindi.append(i.text.replace('\\n',''))\n",
    "    \n",
    "    \n",
    "release_year_hindi=[]\n",
    "for i in soup_imdb_hindi.find_all('span',class_=\"lister-item-year text-muted unbold\"):\n",
    "    release_year_hindi.append(i.text)\n",
    "\n",
    "data={'Movie':movie_list_hindi,'Rating':rating_list_hindi,'Release Year':release_year_hindi}\n",
    "\n",
    "df=pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051e7b5e",
   "metadata": {},
   "source": [
    "# Meesho products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "page_meesho= requests.get('https://meesho.com/bags-ladies/pl/p7vbp')\n",
    "soup_meesho= BeautifulSoup(page_meesho.text, 'html.parser')\n",
    "\n",
    "product_name=[]\n",
    "for i in soup_meesho.find_all(class_=\"Text__StyledText-sc-oo0kvp-0 bWSOET NewProductCard__ProductTitle_Desktop-sc-j0e7tu-4 cQhePS NewProductCard__ProductTitle_Desktop-sc-j0e7tu-4 cQhePS\"):\n",
    "    product_name.append(i.text)\n",
    "    \n",
    "price=[]\n",
    "for i in soup_meesho.find_all(class_=\"Text__StyledText-sc-oo0kvp-0 hiHdyy\"):\n",
    "    price.append(i.text.replace('onwards',''))\n",
    "    \n",
    "discount=[]\n",
    "for i in soup_meesho.find_all(class_=\"Text__StyledText-sc-oo0kvp-0 lnonyH\"):\n",
    "    discount.append(i.text.replace('off',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd59979",
   "metadata": {},
   "source": [
    "# ICC Top 10 ODI teams-men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20482ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "page_icc_men= requests.get('https://www.icc-cricket.com/rankings/mens/team-rankings/odi')\n",
    "soup_icc_men= BeautifulSoup(page_icc_men.text, 'html.parser')\n",
    "\n",
    "team_men=[]\n",
    "for i in soup_icc_men.find_all('span',class_=\"u-hide-phablet\"):\n",
    "    team_men.append(i.text)\n",
    "    \n",
    "team_men_matches=[]\n",
    "for i in soup_icc_men.find_all('td',class_=\"table-body__cell u-center-text\"):\n",
    "    team_men_matches.append(i.text)\n",
    "    \n",
    "team_men_points=[]\n",
    "for i in soup_icc_men.find_all('td',class_=\"table-body__cell u-center-text\"):\n",
    "    team_men_points.append(i.text)\n",
    "\n",
    "team_men_rating=[]\n",
    "for i in soup_icc_men.find_all('td',class_=\"table-body__cell u-text-right rating\"):\n",
    "    team_men_rating.append(i.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe113c37",
   "metadata": {},
   "source": [
    "# ICC Top 10 ODI teams-women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5bce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "page_icc_women= requests.get('https://www.icc-cricket.com/rankings/womens/team-rankings/odi')\n",
    "soup_icc_women= BeautifulSoup(page_icc_women.text, 'html.parser')\n",
    "\n",
    "team_women=[]\n",
    "for i in soup_icc_women.find_all('span',class_=\"u-hide-phablet\"):\n",
    "    team_women.append(i.text)\n",
    "    \n",
    "team_women_matches=[]\n",
    "for i in soup_icc_women.find_all('td',class_=\"table-body__cell u-center-text\"):\n",
    "    team_women_matches.append(i.text)\n",
    "\n",
    "team_women_points=[]\n",
    "for i in soup_icc_women.find_all('td',class_=\"table-body__cell u-center-text\"):\n",
    "    team_women_points.append(i.text)\n",
    "\n",
    "team_women_rating=[]\n",
    "for i in soup_icc_women.find_all('td',class_=\"table-body__cell u-text-right rating\"):\n",
    "    team_women_rating.append(i.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79c5220",
   "metadata": {},
   "source": [
    "# CoreyMS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99077f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "page_corev=requests.get('https://coreyms.com/')\n",
    "soup_corev= BeautifulSoup(page_corev.text, 'html.parser')\n",
    "\n",
    "heading=[]\n",
    "for i in soup_corev.find_all(class_=\"entry-title-link\"):\n",
    "    heading.append(i.text)\n",
    "\n",
    "date=[]\n",
    "for i in soup_corev.find_all(class_=\"entry-time\"):\n",
    "    date.append(i.text)\n",
    "\n",
    "content=[]\n",
    "for i in soup_corev.find_all(class_=\"entry-content\"):\n",
    "    content.append(i.text)\n",
    "\n",
    "video=[]\n",
    "for i in soup_corev.find_all('a'):\n",
    "   video.append(i.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d5feb4",
   "metadata": {},
   "source": [
    "# House data_nobroker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461cf775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "page_house=requests.get('https://www.nobroker.in/property/sale/bangalore/multiple?searchParam=W3sibGF0IjoxMi45NzgzNjkyLCJsb24iOjc3LjY0MDgzNTYsInBsYWNlSWQiOiJDaElKa1FOM0dLUVdyanNSTmhCUUpyaEdEN1UiLCJwbGFjZU5hbWUiOiJJbmRpcmFuYWdhciIsInNob3dNYXAiOmZhbHNlfSx7ImxhdCI6MTIuOTMwNzczNSwibG9uIjo3Ny41ODM4MzAyLCJwbGFjZUlkIjoiQ2hJSjJkZGxaNWdWcmpzUmgxQk9BYWYtb3JzIiwicGxhY2VOYW1lIjoiSmF5YW5hZ2FyIiwic2hvd01hcCI6ZmFsc2V9LHsibGF0IjoxMi45OTgxNzMyLCJsb24iOjc3LjU1MzA0NDU5OTk5OTk5LCJwbGFjZUlkIjoiQ2hJSnhmVzREUE05cmpzUktzTlRHLTVwX1FRIiwicGxhY2VOYW1lIjoiUmFqYWppbmFnYXIiLCJzaG93TWFwIjpmYWxzZX1d')\n",
    "soup_house= BeautifulSoup(page_house.text, 'html.parser')\n",
    "\n",
    "house_title=[]\n",
    "for i in soup_house.find_all('span',class_=\"overflow-hidden overflow-ellipsis whitespace-nowrap max-w-80pe po:max-w-full\"):\n",
    "    house_title.append(i.text)\n",
    "\n",
    "house_location=[]\n",
    "for i in soup_house.find_all('div',class_=\"mt-0.5p overflow-hidden overflow-ellipsis whitespace-nowrap max-w-70 text-gray-light leading-4 po:mb-0 po:max-w-95\"):\n",
    "    house_location.append(i.text)\n",
    "    \n",
    "house_area=[]\n",
    "for i in soup_house.find_all('div',class_=\"flex\"):\n",
    "    house_area.append(i.text)\n",
    "    \n",
    "house_EMI=[]\n",
    "for i in soup_house.find_all('div',class_=\"font-semi-bold heading-6\"):\n",
    "    house_EMI.append(i.text)\n",
    "    \n",
    "house_price=[]\n",
    "for i in soup_house.find_all('div',class_=\"font-semi-bold heading-6\"):\n",
    "    house_price.append(i.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93e2479",
   "metadata": {},
   "source": [
    "# Dineout Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4fb0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "page_dine=requests.get('https://www.dineout.co.in/delhi-restaurants/buffet-special')\n",
    "soup_dine= BeautifulSoup(page_dine.text, 'html.parser')\n",
    "\n",
    "resto_name=[]\n",
    "for i in soup_dine.find_all('div',class_=\"restnt-info cursor\"):\n",
    "    resto_name.append(i.text)\n",
    "\n",
    "resto_location=[]\n",
    "for i in soup_dine.find_all('div',class_=\"restnt-loc ellipsis\"):\n",
    "    resto_location.append(i.text)\n",
    "    \n",
    "resto_rating=[]\n",
    "for i in soup_dine.find_all('div',class_=\"restnt-rating rating-4\"):\n",
    "    resto_rating.append(i.text)\n",
    "\n",
    "resto_image_url=[]\n",
    "for i in soup_dine.find_all('img',class_=\"no-img\"):\n",
    "    resto_image_url.append(i.get('data-src'))\n",
    "    \n",
    "data={'Resto-Name':resto_name,'Location':resto_location,'Rating':resto_rating,'Image':resto_image_url}\n",
    "\n",
    "df=pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a313e8f4",
   "metadata": {},
   "source": [
    "# Bewakoof Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ca71fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "page_bewakoof=requests.get('https://www.bewakoof.com/women-tshirts?ga_q=tshirts')\n",
    "soup_bewakoof= BeautifulSoup(page_bewakoof.text, 'html.parser')\n",
    "\n",
    "product_name=[]\n",
    "for i in soup_bewakoof.find_all('h3'):\n",
    "    product_name.append(i.text)\n",
    "\n",
    "product_price=[]\n",
    "for i in soup_bewakoof.find_all('b'):\n",
    "    product_price.append(i.text)\n",
    "\n",
    "product_image=[]\n",
    "for i in soup_bewakoof.find_all(class_=\"productImgTag\"):\n",
    "    product_image.append(i.get('src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c59189e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef45f48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
